<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Ollama: Why and How to Use It | BLOG</title>
<meta name="keywords" content="Ollama, AI Models, Open Source, Coding, Productivity">
<meta name="description" content="A comprehensive guide to Ollama — what it is, why it matters, and how to download, install, and use it effectively for coding and AI projects.">
<meta name="author" content="">
<link rel="canonical" href="/posts/ollamarunai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fd5566c526ae48aadabd950798a2dc3568536401560eae5caac3765a42a9e7b5.css" integrity="sha256-/VVmxSauSKravZUHmKLcNWhTZAFWDq5cqsN2WkKp57U=" rel="preload stylesheet" as="style">
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="/posts/ollamarunai/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="/posts/ollamarunai/">
  <meta property="og:site_name" content="BLOG">
  <meta property="og:title" content="Ollama: Why and How to Use It">
  <meta property="og:description" content="A comprehensive guide to Ollama — what it is, why it matters, and how to download, install, and use it effectively for coding and AI projects.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-08T10:30:00+05:30">
    <meta property="article:modified_time" content="2025-11-08T10:30:00+05:30">
    <meta property="article:tag" content="Ollama">
    <meta property="article:tag" content="AI Models">
    <meta property="article:tag" content="Open Source">
    <meta property="article:tag" content="Coding">
    <meta property="article:tag" content="Productivity">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ollama: Why and How to Use It">
<meta name="twitter:description" content="A comprehensive guide to Ollama — what it is, why it matters, and how to download, install, and use it effectively for coding and AI projects.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "/posts/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ollama: Why and How to Use It",
      "item": "/posts/ollamarunai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ollama: Why and How to Use It",
  "name": "Ollama: Why and How to Use It",
  "description": "A comprehensive guide to Ollama — what it is, why it matters, and how to download, install, and use it effectively for coding and AI projects.",
  "keywords": [
    "Ollama", "AI Models", "Open Source", "Coding", "Productivity"
  ],
  "articleBody": "Ollama: Why and How to Use It Artificial Intelligence is transforming the way we develop software, learn programming, and experiment with new ideas.\nOllama is an AI platform that allows developers and students to run AI models locally, enabling offline usage, improved privacy, and full control over their AI workflow.\nThis guide explains why Ollama is important, how to download and install it, and how to integrate it into coding projects.\nWhy Ollama Matters Many AI services require cloud access, subscriptions, or constant internet connectivity. This creates limitations:\nDependency on external servers Limited access due to cost or usage limits Privacy concerns with sensitive data Ollama solves these issues by allowing local execution of AI models, giving users:\nOffline capabilities: Run AI models without internet. Privacy: All data stays on your machine. Flexibility: Use AI in scripts, applications, and learning environments. Learning opportunities: Explore AI without restrictions. Who Should Use Ollama Ollama is ideal for:\nStudents learning AI Independent developers experimenting with code generation Hobbyists building offline AI-powered apps Researchers needing control and transparency It bridges the gap between accessibility, power, and privacy, which is often missing in cloud-based AI services.\nHow to Download and Install Ollama Step 1: Visit the Official Website Go to the official Ollama site:\nhttps://ollama.com\nStep 2: Choose Your Platform Ollama supports multiple platforms, including:\nWindows macOS Linux Download the version that matches your operating system.\nStep 3: Install the Application Windows Open the .exe installer file. Follow the prompts to install. Verify installation by opening a terminal and running: ollama --version macOS Open the .dmg file. Drag Ollama to the Applications folder. Verify installation using: ollama --version Linux Extract the downloaded package. Run the installer script: sudo ./install.sh Verify installation: ollama --version Setting Up Ollama for the First Time Once installed, you may need to download AI models to use locally.\nStep 1: List Available Models ollama list This command shows all models available for download.\nStep 2: Download a Model ollama pull For example:\nollama pull llama2-7b This downloads the model to your machine for offline use.\nStep 3: Verify the Model ollama run This opens an interactive session with the model.\nUsing Ollama with Code Ollama can be integrated into your Python, Node.js, or shell scripts to enhance coding productivity.\nExample: Using Ollama in Python Install the Python wrapper (if available) or call via subprocess: import subprocess # Run Ollama model and get response result = subprocess.run( [\"ollama\", \"run\", \"llama2-7b\", \"--prompt\", \"Write a Python function to reverse a string.\"], capture_output=True, text=True ) print(result.stdout) This allows automated AI-assisted code generation directly in your Python projects.\nExample: Command-Line Prompt ollama run llama2-7b --prompt \"Explain the merge sort algorithm in simple terms.\" You can redirect the output to a file or use it as input in other scripts.\nBest Practices for Students and Developers Start small: Begin with a single model and simple prompts. Offline experimentation: Use Ollama for coding practice without worrying about cloud limits. Integrate with your workflow: Add Ollama to scripts, project templates, or custom coding tools. Respect resources: Some models are large and may require significant disk space and memory. Document prompts: Keep track of prompts and outputs for future reference. Why Ollama Is a Game-Changer Full control over AI: No cloud dependency. Educational value: Learn AI by running models locally. Privacy and security: Sensitive data never leaves your system. Open experimentation: Test, modify, and explore models freely. For students and independent developers, Ollama democratizes AI, making it a tool for everyone, not just organizations with budgets or cloud access.\nClosing Thoughts Ollama represents a new era in AI accessibility. By combining local model execution, privacy, and flexibility, it allows developers and students to experiment, learn, and create without constraints.\nWhether you are coding, learning AI, or building offline applications, Ollama is a powerful tool that should be part of your toolkit.\nWritten by Dihan Ramanayaka — Student Developer and Advocate for Open-Source AI Tools.\n",
  "wordCount" : "656",
  "inLanguage": "en",
  "datePublished": "2025-11-08T10:30:00+05:30",
  "dateModified": "2025-11-08T10:30:00+05:30",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/posts/ollamarunai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "BLOG",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="BLOG (Alt + H)">BLOG</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Ollama: Why and How to Use It
    </h1>
    <div class="post-description">
      A comprehensive guide to Ollama — what it is, why it matters, and how to download, install, and use it effectively for coding and AI projects.
    </div>
    <div class="post-meta"><span title='2025-11-08 10:30:00 +0530 +0530'>November 8, 2025</span>

</div>
  </header> 
  <div class="post-content"><h1 id="ollama-why-and-how-to-use-it">Ollama: Why and How to Use It<a hidden class="anchor" aria-hidden="true" href="#ollama-why-and-how-to-use-it">#</a></h1>
<p>Artificial Intelligence is transforming the way we develop software, learn programming, and experiment with new ideas.<br>
<strong>Ollama</strong> is an AI platform that allows developers and students to <strong>run AI models locally</strong>, enabling offline usage, improved privacy, and full control over their AI workflow.</p>
<p>This guide explains <strong>why Ollama is important, how to download and install it, and how to integrate it into coding projects</strong>.</p>
<hr>
<h2 id="why-ollama-matters">Why Ollama Matters<a hidden class="anchor" aria-hidden="true" href="#why-ollama-matters">#</a></h2>
<p>Many AI services require cloud access, subscriptions, or constant internet connectivity. This creates limitations:</p>
<ul>
<li><strong>Dependency on external servers</strong></li>
<li><strong>Limited access due to cost or usage limits</strong></li>
<li><strong>Privacy concerns with sensitive data</strong></li>
</ul>
<p>Ollama solves these issues by allowing <strong>local execution of AI models</strong>, giving users:</p>
<ul>
<li><strong>Offline capabilities</strong>: Run AI models without internet.</li>
<li><strong>Privacy</strong>: All data stays on your machine.</li>
<li><strong>Flexibility</strong>: Use AI in scripts, applications, and learning environments.</li>
<li><strong>Learning opportunities</strong>: Explore AI without restrictions.</li>
</ul>
<hr>
<h2 id="who-should-use-ollama">Who Should Use Ollama<a hidden class="anchor" aria-hidden="true" href="#who-should-use-ollama">#</a></h2>
<p>Ollama is ideal for:</p>
<ul>
<li><strong>Students learning AI</strong></li>
<li><strong>Independent developers experimenting with code generation</strong></li>
<li><strong>Hobbyists building offline AI-powered apps</strong></li>
<li><strong>Researchers needing control and transparency</strong></li>
</ul>
<p>It bridges the gap between <strong>accessibility, power, and privacy</strong>, which is often missing in cloud-based AI services.</p>
<hr>
<h2 id="how-to-download-and-install-ollama">How to Download and Install Ollama<a hidden class="anchor" aria-hidden="true" href="#how-to-download-and-install-ollama">#</a></h2>
<h3 id="step-1-visit-the-official-website">Step 1: Visit the Official Website<a hidden class="anchor" aria-hidden="true" href="#step-1-visit-the-official-website">#</a></h3>
<p>Go to the official Ollama site:<br>
<a href="https://ollama.com">https://ollama.com</a></p>
<h3 id="step-2-choose-your-platform">Step 2: Choose Your Platform<a hidden class="anchor" aria-hidden="true" href="#step-2-choose-your-platform">#</a></h3>
<p>Ollama supports multiple platforms, including:</p>
<ul>
<li><strong>Windows</strong></li>
<li><strong>macOS</strong></li>
<li><strong>Linux</strong></li>
</ul>
<p>Download the version that matches your operating system.</p>
<h3 id="step-3-install-the-application">Step 3: Install the Application<a hidden class="anchor" aria-hidden="true" href="#step-3-install-the-application">#</a></h3>
<h4 id="windows">Windows<a hidden class="anchor" aria-hidden="true" href="#windows">#</a></h4>
<ol>
<li>Open the <code>.exe</code> installer file.</li>
<li>Follow the prompts to install.</li>
<li>Verify installation by opening a terminal and running:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama --version
</span></span></code></pre></div><h4 id="macos">macOS<a hidden class="anchor" aria-hidden="true" href="#macos">#</a></h4>
<ol>
<li>Open the <code>.dmg</code> file.</li>
<li>Drag Ollama to the Applications folder.</li>
<li>Verify installation using:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama --version
</span></span></code></pre></div><h4 id="linux">Linux<a hidden class="anchor" aria-hidden="true" href="#linux">#</a></h4>
<ol>
<li>Extract the downloaded package.</li>
<li>Run the installer script:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo ./install.sh
</span></span></code></pre></div><ol start="3">
<li>Verify installation:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama --version
</span></span></code></pre></div><hr>
<h2 id="setting-up-ollama-for-the-first-time">Setting Up Ollama for the First Time<a hidden class="anchor" aria-hidden="true" href="#setting-up-ollama-for-the-first-time">#</a></h2>
<p>Once installed, you may need to <strong>download AI models</strong> to use locally.</p>
<h3 id="step-1-list-available-models">Step 1: List Available Models<a hidden class="anchor" aria-hidden="true" href="#step-1-list-available-models">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama list
</span></span></code></pre></div><p>This command shows all models available for download.</p>
<h3 id="step-2-download-a-model">Step 2: Download a Model<a hidden class="anchor" aria-hidden="true" href="#step-2-download-a-model">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama pull &lt;model_name&gt;
</span></span></code></pre></div><p>For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama pull llama2-7b
</span></span></code></pre></div><p>This downloads the model to your machine for offline use.</p>
<h3 id="step-3-verify-the-model">Step 3: Verify the Model<a hidden class="anchor" aria-hidden="true" href="#step-3-verify-the-model">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run &lt;model_name&gt;
</span></span></code></pre></div><p>This opens an interactive session with the model.</p>
<hr>
<h2 id="using-ollama-with-code">Using Ollama with Code<a hidden class="anchor" aria-hidden="true" href="#using-ollama-with-code">#</a></h2>
<p>Ollama can be integrated into your <strong>Python, Node.js, or shell scripts</strong> to enhance coding productivity.</p>
<h3 id="example-using-ollama-in-python">Example: Using Ollama in Python<a hidden class="anchor" aria-hidden="true" href="#example-using-ollama-in-python">#</a></h3>
<ol>
<li>Install the Python wrapper (if available) or call via subprocess:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> subprocess
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run Ollama model and get response</span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> subprocess<span style="color:#f92672">.</span>run(
</span></span><span style="display:flex;"><span>    [<span style="color:#e6db74">&#34;ollama&#34;</span>, <span style="color:#e6db74">&#34;run&#34;</span>, <span style="color:#e6db74">&#34;llama2-7b&#34;</span>, <span style="color:#e6db74">&#34;--prompt&#34;</span>, <span style="color:#e6db74">&#34;Write a Python function to reverse a string.&#34;</span>],
</span></span><span style="display:flex;"><span>    capture_output<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    text<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(result<span style="color:#f92672">.</span>stdout)
</span></span></code></pre></div><p>This allows <strong>automated AI-assisted code generation</strong> directly in your Python projects.</p>
<h3 id="example-command-line-prompt">Example: Command-Line Prompt<a hidden class="anchor" aria-hidden="true" href="#example-command-line-prompt">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run llama2-7b --prompt <span style="color:#e6db74">&#34;Explain the merge sort algorithm in simple terms.&#34;</span>
</span></span></code></pre></div><p>You can redirect the output to a file or use it as input in other scripts.</p>
<hr>
<h2 id="best-practices-for-students-and-developers">Best Practices for Students and Developers<a hidden class="anchor" aria-hidden="true" href="#best-practices-for-students-and-developers">#</a></h2>
<ul>
<li><strong>Start small</strong>: Begin with a single model and simple prompts.</li>
<li><strong>Offline experimentation</strong>: Use Ollama for coding practice without worrying about cloud limits.</li>
<li><strong>Integrate with your workflow</strong>: Add Ollama to scripts, project templates, or custom coding tools.</li>
<li><strong>Respect resources</strong>: Some models are large and may require significant disk space and memory.</li>
<li><strong>Document prompts</strong>: Keep track of prompts and outputs for future reference.</li>
</ul>
<hr>
<h2 id="why-ollama-is-a-game-changer">Why Ollama Is a Game-Changer<a hidden class="anchor" aria-hidden="true" href="#why-ollama-is-a-game-changer">#</a></h2>
<ol>
<li><strong>Full control</strong> over AI: No cloud dependency.</li>
<li><strong>Educational value</strong>: Learn AI by running models locally.</li>
<li><strong>Privacy and security</strong>: Sensitive data never leaves your system.</li>
<li><strong>Open experimentation</strong>: Test, modify, and explore models freely.</li>
</ol>
<p>For students and independent developers, Ollama <strong>democratizes AI</strong>, making it a tool for everyone, not just organizations with budgets or cloud access.</p>
<hr>
<h2 id="closing-thoughts">Closing Thoughts<a hidden class="anchor" aria-hidden="true" href="#closing-thoughts">#</a></h2>
<p>Ollama represents a <strong>new era in AI accessibility</strong>. By combining local model execution, privacy, and flexibility, it allows developers and students to <strong>experiment, learn, and create</strong> without constraints.</p>
<p>Whether you are coding, learning AI, or building offline applications, Ollama is a powerful tool that should be part of your toolkit.</p>
<hr>
<p><em>Written by Dihan Ramanayaka — Student Developer and Advocate for Open-Source AI Tools.</em></p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="/tags/ollama/">Ollama</a></li>
      <li><a href="/tags/ai-models/">AI Models</a></li>
      <li><a href="/tags/open-source/">Open Source</a></li>
      <li><a href="/tags/coding/">Coding</a></li>
      <li><a href="/tags/productivity/">Productivity</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="/">BLOG</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
